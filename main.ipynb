{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868c1e99-5f27-4d24-98e8-3a2e9265535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Spacer, Image as ReportLabImage, Paragraph\n",
    "from reportlab.lib.enums import TA_CENTER\n",
    "from llmsherpa.readers import LayoutPDFReader\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from PIL import Image as PILImage\n",
    "from spire.pdf import PdfDocument\n",
    "from spire.pdf.common import ImageFormat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3ceed-6fba-496d-abe9-36e6533b0e7d",
   "metadata": {},
   "source": [
    "##### Downloading PDF file and initialization of tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1cea73d-a3e2-4a5d-ad27-1c1cabdfc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = 'https://arxiv.org/pdf/1706.03762.pdf'\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "text = str(loader.load())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1037f8-b9ca-4afd-a8ab-98457867a27d",
   "metadata": {},
   "source": [
    "If you dont need also extratrion just change value to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbd435d-ea4d-48ba-a26f-5e492986a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_extr = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14197a-f98e-45ef-a321-62277cf6d217",
   "metadata": {},
   "source": [
    "#### Headers extraction and preparation\n",
    "1) extract by using webs crapping\n",
    "2) format headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae9c7d5-a218-4a86-8e16-c1d658fcd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting PDF into HTML to find headers by using HTML tags like <h1>, <h2> etc\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)\n",
    "doc = doc.to_html()\n",
    "soup = BeautifulSoup(doc, 'html.parser')\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'td'])\n",
    "\n",
    "# all headers and other useful lines conteins in headers_text\n",
    "headers_text = [header.get_text(strip=True).strip() for header in headers]\n",
    "headers_text = list(dict.fromkeys(headers_text))\n",
    "\n",
    "# in arXiv documents there are can issues with Introduction header, so if this header not include we add it\n",
    "pattern = re.compile(r'^\\d+(\\.\\d+)*\\s')\n",
    "numbered_headers = [header for header in headers_text if pattern.match(header)]\n",
    "intro_present = any(re.match(r'^1\\s+Introduction', header, re.IGNORECASE) for header in numbered_headers)\n",
    "if not intro_present:\n",
    "    index_for_intro = next((i for i, header in enumerate(numbered_headers) if header.startswith('2 ')), 0)\n",
    "    numbered_headers.insert(index_for_intro, '1 Introduction')\n",
    "\n",
    "# extract all useful headers\n",
    "updated_pattern = re.compile(r'''\n",
    "    ^                         # Start of line\n",
    "    (\\d+(\\.\\d+)*)             # Section or subsection number (e.g., \"3\", \"3.2\", \"3.2.1\")\n",
    "    (\\s+[A-Za-z].*)           # Space and section title starting with a letter\n",
    "    $                         # End of line\n",
    "    |                         # OR\n",
    "    ^\\d+\\s\\d+\\.\\d+\\s          # Starts with numbers separated by spaces, with a dot between numbers\n",
    "    (\\d+\\.\\d+\\s+)*            # Followed by a series of numbers with dots and spaces\n",
    "    \\d+K?\\.\\d+                # Ends with a number with a decimal part, possibly with \"K\"\n",
    "    (\\s\\d+)*                  # Followed by spaces and numbers\n",
    "    $                         # End of line\n",
    "''', re.VERBOSE)\n",
    "headers = [header for header in numbered_headers if updated_pattern.match(header)]\n",
    "#same problem like with Introduction\n",
    "headers.append(\"REFERENCES\" if \"REFERENCES\" in text else \"References\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908f9ff-e64d-4714-a754-2bd99719dc53",
   "metadata": {},
   "source": [
    "#### Photo extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9daac1bb-c8b4-47e6-a211-e9a5e78ef172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if photo_extr:\n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open('arXiv_doc.pdf', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        doc = PdfDocument()\n",
    "        doc.LoadFromFile('arXiv_doc.pdf')\n",
    "        \n",
    "    images = []\n",
    "    \n",
    "    for i in range(doc.Pages.Count):\n",
    "        page = doc.Pages.get_Item(i)\n",
    "        for image in page.ExtractImages():\n",
    "            images.append(image)\n",
    "            \n",
    "    index = 0\n",
    "\n",
    "    images_folder = 'images'\n",
    "    \n",
    "    if not os.path.exists(images_folder):\n",
    "        os.makedirs(images_folder)\n",
    "    \n",
    "    for image in images:\n",
    "        imageFileName = r'images\\image_{0}.png'.format(index).format(index)\n",
    "        index += 1\n",
    "        image.Save(imageFileName, ImageFormat.get_Png())\n",
    "        \n",
    "    doc.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa1dc6-3c1c-4720-b9e0-80b96e5479c5",
   "metadata": {},
   "source": [
    "#### Text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad41a6e-7f3f-4cc9-9d2f-d413fc1cc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerize(text):\n",
    "    inputs_no_trunc = tokenizer(text, max_length=None, return_tensors='pt', truncation=False) \n",
    "    \n",
    "    chunk_start = 0\n",
    "    chunk_end = tokenizer.model_max_length  # == 1024 for Bart\n",
    "    inputs_batch_lst = []\n",
    "    space_token_id = tokenizer.encode(' ', add_special_tokens=False)[0]\n",
    "    \n",
    "    while chunk_start <= len(inputs_no_trunc['input_ids'][0]):\n",
    "        try:\n",
    "            current_chunk = inputs_no_trunc['input_ids'][0][chunk_start:chunk_end].tolist()\n",
    "            end_index = len(current_chunk) - 1 - current_chunk[::-1].index(space_token_id)\n",
    "            chunk_end = chunk_start + end_index\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "        inputs_batch = inputs_no_trunc['input_ids'][0][chunk_start:chunk_end] # get batch of n tokens\n",
    "        inputs_batch = torch.unsqueeze(inputs_batch, 0)\n",
    "        inputs_batch_lst.append(inputs_batch)\n",
    "        chunk_start = chunk_end + 1\n",
    "        chunk_end = min(chunk_start + tokenizer.model_max_length, len(inputs_no_trunc['input_ids'][0]))\n",
    "    \n",
    "    summary_ids_lst = [model.generate(inputs, num_beams=4, max_length=1024, early_stopping=True) for inputs in inputs_batch_lst]\n",
    "                                                                     \n",
    "    summary_batch_lst = []\n",
    "    for summary_id in summary_ids_lst:\n",
    "        summary_batch = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_id]\n",
    "        summary_batch_lst.append(summary_batch[0])\n",
    "    summary_all = '\\n'.join(summary_batch_lst)\n",
    "    return summary_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ad17e-7c9f-45ce-a66a-db1e7099707a",
   "metadata": {},
   "source": [
    "#### Preparation for adding text into a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a34be0-9b3d-480e-b260-b88fa596f95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patternForBrackets = re.compile(r'\\[\\s*\\d+(?:,\\s*\\d+)*\\s*\\]') # in arXiv documents exist links like [2], [3, 1], [12, 45], so we delete them\n",
    "pdf_path = \"summary.pdf\"\n",
    "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
    "styles = getSampleStyleSheet()\n",
    "style = styles['Normal']\n",
    "elements = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07e083-8748-4ec9-9548-f383270b08e3",
   "metadata": {},
   "source": [
    "#### Text adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8de862-969e-45cc-8fa2-af5983ef42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1656 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "for header in range(len(headers)-1):\n",
    "    startHeader = headers[header]\n",
    "    endHeader = headers[header + 1]\n",
    "\n",
    "    # header size depends on subheading it or not\n",
    "    dot_count = startHeader.count('.')\n",
    "    if dot_count == 0:\n",
    "        header_style = styles['Heading2']\n",
    "    elif dot_count != 0:\n",
    "        header_style = styles['Heading5']\n",
    "\n",
    "    # search for text between startHeader and endHeader\n",
    "    pattern = re.compile(re.escape(startHeader) + \"(.*?)\" + re.escape(endHeader), re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    # adding\n",
    "    if match:\n",
    "        text_between = match.group(1)\n",
    "        text_without_brackets = patternForBrackets.sub('', text_between) # deletion of brackets\n",
    "        # same problem with '\\\\n' and '\\n' as with the brackets\n",
    "        text_final = text_without_brackets.replace(\"\\\\n\",\"\") # deletion of '\\\\n' sign\n",
    "        text_final = text_final.replace(\"\\n\",\"\") # deletion of '\\n' sign\n",
    "\n",
    "        summary_text = summerize(text_final)        \n",
    "        elements.append(Paragraph(startHeader, header_style))\n",
    "        elements.append(Paragraph(summary_text, style))\n",
    "        elements.append(Spacer(1, 12))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f7284-b06d-4b61-96a8-8b2575245fc3",
   "metadata": {},
   "source": [
    "#### Photo adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2b0ab61-1480-43f8-ae15-a4aee49b053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if photo_extr:\n",
    "    styles = getSampleStyleSheet()\n",
    "    for img_num in range(len(images)):\n",
    "        image_path = f'images/image_{img_num}.png'\n",
    "        pil_image = PILImage.open(image_path)\n",
    "        real_width, real_height = pil_image.size\n",
    "\n",
    "        # there are can be problems with image size, so we need to prepare it\n",
    "        dpi = 72  \n",
    "        width_in_points = real_width / dpi * 7 # in case if your image is too large just change 7 to 5 or 3\n",
    "        height_in_points = real_height / dpi * 7\n",
    "        width_in_points = float(width_in_points)\n",
    "        height_in_points = float(height_in_points)\n",
    "    \n",
    "        img = ReportLabImage(image_path, width_in_points, height_in_points)\n",
    "        elements.append(img)\n",
    "        \n",
    "        # Subheading adding under the hoto\n",
    "        centered_style = ParagraphStyle(name='CenteredStyle', parent=styles['Normal'], alignment=TA_CENTER)\n",
    "        elements.append(Paragraph(f\"Figure {img_num}\", centered_style))\n",
    "        elements.append(Spacer(1, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa44ee-ef9c-44eb-8f2a-a04e90f235b4",
   "metadata": {},
   "source": [
    "#### Biulding PDF summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97013f1b-6403-412f-8073-c9c11e4209f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.build(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756478f9-05d4-4210-9295-7b27df77eee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
